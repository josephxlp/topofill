{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b99d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from uvars import f1,f2,f3,f4,f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddc0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading raster data...\n",
      "Read 'N10E105_tdem_dem_egm_v_3_gap.tif' with shape (9001, 9001). Nodata value: -9999.0\n",
      "Read 'N10E105_esawc_x.tif' with shape (9001, 9001). Nodata value: -9999.0\n",
      "Read 'N10E105_tdem_hem.tif' with shape (9001, 9001). Nodata value: -9999.0\n",
      "Read 'N10E105_s1.tif' with shape (9001, 9001). Nodata value: -9999.0\n",
      "Read 'N10E105_tdem_dem_egm.tif' with shape (9001, 9001). Nodata value: -9999.0\n",
      "\n",
      "Number of nulls in 'N10E105_tdem_dem_egm_v_3_gap.tif' before imputation: 6149456\n",
      "\n",
      "--- Running Ensemble Component: LGBM ---\n",
      "Initializing estimator for method: 'lgbm'\n",
      "[IterativeImputer] Completing matrix with shape (81018001, 5)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.336859\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.389832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -310.547040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.373575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -10.547778\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.374869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -307.759412\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 74868545, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 3.334202\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 1838.74\n",
      "[IterativeImputer] Change: 316.0906066894531, scaled tolerance: 32.767 \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.282887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.336859\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.380771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -310.547040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.362231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -10.547778\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.378828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 81018001, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -307.759412\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.394508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 773\n",
      "[LightGBM] [Info] Number of data points in the train set: 74868545, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 3.334202\n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 3742.88\n",
      "[IterativeImputer] Change: 0.0, scaled tolerance: 32.767 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "Writing filled raster to 'output_filled_rasters/N10E105_tdem_dem_egm_v_3_gap_filled_lgbm.tif'...\n",
      "Write complete.\n",
      "\n",
      "--- Running Ensemble Component: XGB ---\n",
      "Initializing estimator for method: 'xgb'\n",
      "[IterativeImputer] Completing matrix with shape (81018001, 5)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 6582.56\n",
      "[IterativeImputer] Change: 293.69610595703125, scaled tolerance: 32.767 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 12681.16\n",
      "[IterativeImputer] Change: 0.0, scaled tolerance: 32.767 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "Writing filled raster to 'output_filled_rasters/N10E105_tdem_dem_egm_v_3_gap_filled_xgb.tif'...\n",
      "Write complete.\n",
      "\n",
      "--- Running Ensemble Component: CTB ---\n",
      "Initializing estimator for method: 'ctb'\n",
      "[IterativeImputer] Completing matrix with shape (81018001, 5)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 7539.23\n",
      "[IterativeImputer] Change: 283.25775146484375, scaled tolerance: 32.767 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 14976.19\n",
      "[IterativeImputer] Change: 0.0, scaled tolerance: 32.767 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "Writing filled raster to 'output_filled_rasters/N10E105_tdem_dem_egm_v_3_gap_filled_ctb.tif'...\n",
      "Write complete.\n",
      "\n",
      "Averaging results for ensemble...\n",
      "Writing filled raster to 'output_filled_rasters/N10E105_tdem_dem_egm_v_3_gap_filled_ensemble.tif'...\n",
      "Write complete.\n",
      "\n",
      "--- Process complete. ---\n",
      "Total time taken: 31462.59 seconds.\n",
      "The final imputed raster has been saved as 'output_filled_rasters/N10E105_tdem_dem_egm_v_3_gap_filled_ensemble.tif'.\n",
      "\n",
      "--- Demonstrating Loading and Using a Saved Imputer ---\n",
      "Saved imputer 'output_filled_rasters/imputer_model_xgb.joblib' not found. Run a single model first to create it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn and related models\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "\n",
    "def read_rasters_to_numpy(raster_paths, target_raster_path):\n",
    "    \"\"\"\n",
    "    Reads a list of rasters, flattens them, and stacks them into a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        raster_paths (list): A list of file paths to the raster files.\n",
    "        target_raster_path (str): The path to the specific raster we intend to impute.\n",
    "                                  Its metadata (profile, shape) will be returned.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - np.ndarray: A 2D array where each column is a flattened raster.\n",
    "            - list: A list of column names (the original raster paths).\n",
    "            - dict: The rasterio profile of the target raster.\n",
    "            - tuple: The original (height, width) shape of the target raster.\n",
    "    \"\"\"\n",
    "    raster_data_list = []\n",
    "    column_names = []\n",
    "    target_profile = None\n",
    "    target_shape = None\n",
    "\n",
    "    print(\"\\nReading raster data...\")\n",
    "    for path in raster_paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            if path == target_raster_path:\n",
    "                target_profile = src.profile\n",
    "                target_shape = src.shape\n",
    "                # Ensure nodata value is converted to np.nan for imputation\n",
    "                nodata_val = src.nodata\n",
    "            \n",
    "            band_data = src.read(1).flatten().astype(np.float32)\n",
    "\n",
    "            # Convert the raster's specific nodata value to NaN\n",
    "            if nodata_val is not None:\n",
    "                band_data[band_data == nodata_val] = np.nan\n",
    "            \n",
    "            raster_data_list.append(band_data)\n",
    "            column_names.append(os.path.basename(path))\n",
    "            print(f\"Read '{os.path.basename(path)}' with shape {src.shape}. Nodata value: {nodata_val}\")\n",
    "\n",
    "    # Stack the 1D arrays as columns in a 2D NumPy array\n",
    "    return np.stack(raster_data_list, axis=1), column_names, target_profile, target_shape\n",
    "\n",
    "\n",
    "def get_estimator(method: str):\n",
    "    \"\"\"\n",
    "    Returns a configured scikit-learn compatible estimator based on the method name.\n",
    "    Estimators are configured with parameters for better performance on large datasets.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing estimator for method: '{method}'\")\n",
    "    if method == 'rf':\n",
    "        # RandomForest: Good all-rounder, but can be memory intensive.\n",
    "        return RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            min_samples_leaf=10, # Regularization to prevent overfitting\n",
    "            n_jobs=-1,           # Use all available CPU cores\n",
    "            random_state=42\n",
    "        )\n",
    "    elif method == 'lgbm':\n",
    "        # LightGBM: Fast and memory efficient.\n",
    "        return lgb.LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            n_jobs=-1,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif method == 'xgb':\n",
    "         # XGBoost: Very powerful, 'hist' tree method is fast for large data.\n",
    "        return xgb.XGBRegressor(\n",
    "            n_estimators=1000,\n",
    "            n_jobs=-1,\n",
    "            tree_method='hist', # Much faster than exact method\n",
    "            eta=0.05,           # Learning rate\n",
    "            random_state=42\n",
    "        )\n",
    "    elif method == 'ctb':\n",
    "        # CatBoost: Robust, handles categorical features well (not used here but good practice).\n",
    "        return ctb.CatBoostRegressor(\n",
    "            n_estimators=1000,\n",
    "            thread_count=-1,\n",
    "            verbose=0,          # Suppress verbose output during training\n",
    "            random_state=42\n",
    "        )\n",
    "    elif method == 'mice':\n",
    "        # Default MICE (BayesianRidge). Fast but potentially less accurate than tree models.\n",
    "        return None # IterativeImputer's default\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: '{method}'. Choose from 'rf', 'lgbm', 'xgb', 'ctb', 'mice', 'ensemble'.\")\n",
    "\n",
    "\n",
    "def write_raster(output_path, data_1d, profile, shape):\n",
    "    \"\"\"Writes a 1D NumPy array to a GeoTIFF raster file.\"\"\"\n",
    "    print(f\"Writing filled raster to '{output_path}'...\")\n",
    "    try:\n",
    "        # Reshape the 1D array back to its original 2D raster shape\n",
    "        reshaped_data = data_1d.reshape(shape)\n",
    "\n",
    "        # Update profile to ensure data type matches\n",
    "        profile.update(dtype=reshaped_data.dtype.name)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(reshaped_data, 1)\n",
    "        print(\"Write complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing raster to {output_path}: {e}\")\n",
    "\n",
    "# --- 2. Main Imputation Logic ---\n",
    "\n",
    "def run_imputation_process(raster_paths, target_raster_path, output_dir, method='ensemble', save_policy='last', save_imputer_model=True):\n",
    "    \"\"\"\n",
    "    Main orchestration function for the raster imputation process.\n",
    "\n",
    "    Args:\n",
    "        raster_paths (list): Paths to all input rasters (features + target).\n",
    "        target_raster_path (str): Path to the specific raster to be filled.\n",
    "        output_dir (str): Directory to save output files.\n",
    "        method (str): Imputation method ('rf', 'lgbm', 'xgb', 'ctb', 'mice', 'ensemble').\n",
    "        save_policy (str): 'all' to save intermediate results, 'last' to save only the final result.\n",
    "        save_imputer_model (bool): If True, saves the fitted imputer object for later use.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- Read Data ---\n",
    "    data_np, col_names, profile, shape = read_rasters_to_numpy(raster_paths, target_raster_path)\n",
    "    target_col_index = raster_paths.index(target_raster_path)\n",
    "    \n",
    "    # Report initial nulls\n",
    "    initial_nulls = np.isnan(data_np[:, target_col_index]).sum()\n",
    "    print(f\"\\nNumber of nulls in '{os.path.basename(target_raster_path)}' before imputation: {initial_nulls}\")\n",
    "    if initial_nulls == 0:\n",
    "        print(\"No missing values to impute. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Imputation ---\n",
    "    start_time = time.time()\n",
    "    \n",
    "    base_target_name = os.path.splitext(os.path.basename(target_raster_path))[0]\n",
    "    \n",
    "    if method == 'ensemble':\n",
    "        imputed_target_cols = []\n",
    "        base_methods = ['lgbm', 'xgb', 'ctb']#'rf',\n",
    "        \n",
    "        for m in base_methods:\n",
    "            print(f\"\\n--- Running Ensemble Component: {m.upper()} ---\")\n",
    "            estimator = get_estimator(m)\n",
    "            imputer = IterativeImputer(estimator=estimator, max_iter=10, random_state=42, verbose=2)\n",
    "            \n",
    "            imputed_data = imputer.fit_transform(data_np)\n",
    "            imputed_target_cols.append(imputed_data[:, target_col_index])\n",
    "            \n",
    "            if save_policy == 'all':\n",
    "                output_path = os.path.join(output_dir, f\"{base_target_name}_filled_{m}.tif\")\n",
    "                write_raster(output_path, imputed_data[:, target_col_index], profile, shape)\n",
    "\n",
    "        # Average the results from all models\n",
    "        print(\"\\nAveraging results for ensemble...\")\n",
    "        final_imputed_target = np.mean(np.stack(imputed_target_cols, axis=1), axis=1)\n",
    "\n",
    "    else: # Single model logic\n",
    "        estimator = get_estimator(method)\n",
    "        imputer = IterativeImputer(estimator=estimator, max_iter=10, random_state=42, verbose=2)\n",
    "        imputed_data = imputer.fit_transform(data_np)\n",
    "        final_imputed_target = imputed_data[:, target_col_index]\n",
    "\n",
    "        if save_imputer_model:\n",
    "            imputer_path = os.path.join(output_dir, f\"imputer_model_{method}.joblib\")\n",
    "            print(f\"\\nSaving fitted imputer to '{imputer_path}'...\")\n",
    "            joblib.dump(imputer, imputer_path)\n",
    "\n",
    "\n",
    "    # --- Write Final Output ---\n",
    "    final_output_path = os.path.join(output_dir, f\"{base_target_name}_filled_{method}.tif\")\n",
    "    write_raster(final_output_path, final_imputed_target, profile, shape)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"\\n--- Process complete. ---\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"The final imputed raster has been saved as '{final_output_path}'.\")\n",
    "\n",
    "\n",
    "# --- 3. Example Usage ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Run the Imputation ---\n",
    "    \n",
    "    # Define paths and output directory\n",
    "    all_raster_paths = [f1, f2, f3, f4,f5]\n",
    "    target_to_fill = f1\n",
    "    output_directory = 'output_filled_rasters'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # === SCENARIO 1: Run a single powerful model (e.g., XGBoost) ===\n",
    "    # run_imputation_process(\n",
    "    #     raster_paths=all_raster_paths,\n",
    "    #     target_raster_path=target_to_fill,\n",
    "    #     output_dir=output_directory,\n",
    "    #     method='xgb', # Choose from 'rf', 'lgbm', 'xgb', 'ctb', 'mice'\n",
    "    #     save_policy='last',\n",
    "    #     save_imputer_model=True\n",
    "    # )\n",
    "\n",
    "    # === SCENARIO 2: Run the robust ENSEMBLE model ===\n",
    "    run_imputation_process(\n",
    "        raster_paths=all_raster_paths,\n",
    "        target_raster_path=target_to_fill,\n",
    "        output_dir=output_directory,\n",
    "        method='ensemble',\n",
    "        save_policy='all', # 'all' saves rf, lgbm, etc. outputs; 'last' saves only ensemble.\n",
    "        save_imputer_model=False # Saving an ensemble imputer is not applicable here\n",
    "    )\n",
    "\n",
    "    # --- Example of Loading a Saved Imputer ---\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it took 9h to run all of the in CPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
